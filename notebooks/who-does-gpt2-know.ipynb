{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "cd /content && rm -rf /content/rome\n",
    "git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n",
    "ln -s rome/dsets .\n",
    "pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "try:\n",
    "    import google.colab, torch, sys\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Change runtime type to include a GPU.\")\n",
    "    sys.path.append('/content/rome')\n",
    "    IS_COLAB = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who does GPT-2 know?\n",
    "\n",
    "A puzzle in this notebook.  If we generate a random person name, can we determine the difference between who GPT-2 specifically knows about, and who it doesn't?\n",
    "\n",
    "Some ideas to try:\n",
    "\n",
    "   * If a name is known, it will have lower perplexity than unknown names.  But I guess that a very common-sounding name of an unknown person might also have low perplexity.  Similarly a known person with an unusual name that is rarely mentioned could have high perplexity.\n",
    "   * If a person is known, then paraphrased prompts asking about the person should produce similar information, rather than changing fundamental details.\n",
    "   * If a person is known, then maybe Causal Tracing would reveal a retrieval of information that is specific to their name.\n",
    "   * If a person is unknown, then maybe changing key facts in a sentence about them could be done without a major change in perplexity, while for a known person, such changes might be penalized strongly.\n",
    "   * If a person is unknown, then similarly maybe changing their name in a sentence might not be penalized very strongly.\n",
    "   * If a person is known, then genuinely true facts about the person should be generated, or at least scored higher than false asserions.\n",
    "   * If a person is known, then maybe there will be large parameters in the model that mediate processing of the person's name.  (On the theory that regularizers will tend to shrink parameters for unseen cases.)\n",
    "   * If a name is known, then given an distinguishing attribute about the person and a portion of their name, the model should be able to elicit the rest of their name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing several utility functions that deal with tokens and transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import torch, numpy\n",
    "from collections import defaultdict\n",
    "from util import nethook\n",
    "from util.globals import DATA_DIR\n",
    "from experiments.causal_trace import ModelAndTokenizer, layername, guess_subject, plot_trace_heatmap\n",
    "from experiments.causal_trace import make_inputs, decode_tokens, find_token_range\n",
    "from experiments.causal_trace import predict_token, predict_from_input\n",
    "from dsets import KnownsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load a model and tokenizer.  We use gpt2-xl, but we can also load much larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model_name = r\"gpt2-xl\"\n",
    "\n",
    "# Note that if you trace other models, you should set noise_level appropriately.\n",
    "# (We use 0.03 for gpt-neox-20b and 0.025 for gpt-j-6b)\n",
    "#model_name = r\"EleutherAI/gpt-neox-20b\"\n",
    "#model_name = r\"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "torch_dtype = torch.float16 if '20b' in model_name else None\n",
    "\n",
    "mt = ModelAndTokenizer(model_name, low_cpu_mem_usage=IS_COLAB, torch_dtype=torch_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating probablity of a single name\n",
    "\n",
    "One idea (see https://www.overleaf.com/read/shbgwqhkrgcd) is that a model's knowlege of a subject can be estimated by asking the model how likely it things the appearance of the subject is.  If it's more likely, then (maybe) the subject has been observed more frequently during training and so knowledge is grounded in observations.\n",
    "\n",
    "So here we just feed a name to the model and ask, what is the estimated negative log probability of that name (summing up the log probabilities).  The lower, the more common the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = make_inputs(mt.tokenizer, ['<|endoftext|>LeBron James'])\n",
    "out = mt.model(**inp)[\"logits\"]\n",
    "tokens = [mt.tokenizer.decode(t) for t in inp['input_ids'][0,1:]]\n",
    "logprobs = -torch.gather(torch.log_softmax(out, -1)[:,:-1], 2, inp['input_ids'][:,1:,None])[0,:,0]\n",
    "for t, lp in zip(tokens, logprobs):\n",
    "    print(t, lp.item())\n",
    "print('Total:', sum(logprobs).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of scoring a raw name, I found it works a little better to prefix the name with some text that might be used to introduce an webpage about a real person, using the word \"About\", as in: \"About LeBron James\".\n",
    "\n",
    "This function grabs the negative log probability of a piece of text after this prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob_of(text):\n",
    "    prefix_size = 1\n",
    "    inp = make_inputs(mt.tokenizer, ['About ' + text])\n",
    "    out = mt.model(**inp)[\"logits\"]\n",
    "    #tokens = [mt.tokenizer.decode(t) for t in inp['input_ids'][0,prefix_size:]]\n",
    "    logprobs = -torch.gather(torch.log_softmax(out, -1)[:,:-prefix_size],\n",
    "                             2, inp['input_ids'][:,prefix_size:,None])[0,:,0]\n",
    "    #for t, lp in zip(tokens, logprobs):\n",
    "    #    print(t, lp.item())\n",
    "    return sum(logprobs).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating candidate names\n",
    "\n",
    "Now based on a list of 200 of the most common U.S. first-names and 100 of the most common U.S. last-names, I just generate 20,000 distinct random names.\n",
    "\n",
    "Then I score them by the logprob function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boy_names = (\n",
    "    'James Robert John Michael David William Richard Joseph Thomas Charles Christopher Daniel Matthew Anthony Mark Donald Steven Paul Andrew Joshua Kenneth Kevin Brian George Timothy Ronald Edward Jason Jeffrey Ryan Jacob Gary Nicholas Eric Jonathan Stephen Larry Justin Scott Brandon Benjamin Samuel Gregory Alexander Frank Patrick Raymond Jack Dennis Jerry Tyler Aaron Jose Adam Nathan Henry Douglas Zachary Peter Kyle Ethan Walter Noah Jeremy Christian Keith Roger Terry Gerald Harold Sean Austin Carl Arthur Lawrence Dylan Jesse Jordan Bryan Billy Joe Bruce Gabriel Logan Albert Willie Alan Juan Wayne Elijah Randy Roy Vincent Ralph Eugene Russell Bobby Mason Philip Louis'\n",
    ").split()\n",
    "girl_names = (\n",
    "    'Mary Patricia Jennifer Linda Elizabeth Barbara Susan Jessica Sarah Karen Lisa Nancy Betty Margaret Sandra Ashley Kimberly Emily Donna Michelle Carol Amanda Dorothy Melissa Deborah Stephanie Rebecca Sharon Laura Cynthia Kathleen Amy Angela Shirley Anna Brenda Pamela Emma Nicole Helen Samantha Katherine Christine Debra Rachel Carolyn Janet Catherine Maria Heather Diane Ruth Julie Olivia Joyce Virginia Victoria Kelly Lauren Christina Joan Evelyn Judith Megan Andrea Cheryl Hannah Jacqueline Martha Gloria Teresa Ann Sara Madison Frances Kathryn Janice Jean Abigail Alice Julia Judy Sophia Grace Denise Amber Doris Marilyn Danielle Beverly Isabella Theresa Diana Natalie Brittany Charlotte Marie Kayla Alexis Lori'\n",
    ").split()\n",
    "\n",
    "first_names = boy_names + girl_names\n",
    "last_names = (\n",
    "    'Smith Johnson Williams Brown Jones Garcia Miller Davis Rodriguez Martinez Hernandez Lopez Gonzales Wilson Anderson Thomas Taylor Moore Jackson Martin Lee Perez Thompson White Harris Sanchez Clark Ramirez Lewis Robinson Walker Young Allen King Wright Scott Torres Nguyen Hill Flores Green Adams Nelson Baker Hall Rivera Campbell Mitchell Carter Roberts Gomez Phillips Evans Turner Diaz Parker Cruz Edwards Collins Reyes Stewart Morris Morales Murphy Cook Rogers Gutierrez Ortiz Morgan Cooper Peterson Bailey Reed Kelly Howard Ramos Kim Cox Ward Richardson Watson Brooks Chavez Wood James Bennett Gray Mendoza Ruiz Hughes Price Alvarez Castillo Sanders Patel Myers Long Ross Foster Jimenez'\n",
    ").split()\n",
    "\n",
    "results = []\n",
    "for f in first_names:\n",
    "    for l in last_names:\n",
    "        n = f'{f} {l}'\n",
    "        p = logprob_of(n)\n",
    "        results.append((p, n))\n",
    "    print(p, n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sort names by the log prob, then several well-known names rise to the top: for example famous author Stephen King, famous tastemaker Martha Stewart, famous rock star Michael Jackson.  Gary Johnson was the Libertarian nominee for U.S. president a few years ago; Russell Wilson is a football player...  Walter White is a fictional character on a popular TV show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known = sorted(results)\n",
    "len(known)\n",
    "print(known[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking consistency of knowledge via prompts\n",
    "\n",
    "Another approach is to test knowledge by getting the model to generate text about the person.  If slight changes in wording cause the model to predict different professions, then it might be a clue that the model doesn't know about the person.\n",
    "\n",
    "Here are a few examples.\n",
    "\n",
    "Here are a couple interesitng cases where the \"consistency test\" seems to contradict the \"log prob\" test.\n",
    "\n",
    "   * Beverly Cooper does not score very well on log prob (19.87), but seems to be recognized as a Hollywood actreess (which is true).  Does the model know who she is?\n",
    "   * Convsersely \"Mark Allen\" scores very well on log prob (13.76), but the model doesn't recognize the athlete or actor by that name, switching between assertions that he is an author, photographer, or in the computer field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    '. {}, the well-known',\n",
    "    '. {} is a professional',\n",
    "    '. {} is the consummate',\n",
    "    '. {} is a professional in the field of',\n",
    "    #'. {} is known for work in the field of',\n",
    "    #'. {} works in the domain of',\n",
    "    #'{} works as a ',\n",
    "]\n",
    "for k in known:\n",
    "    prompts = [t.format(k[1]) for t in templates]\n",
    "    #print(prompts)\n",
    "    #for t in templates:\n",
    "    #x    print(t.format(k[1]))\n",
    "    print(k, ' '.join([predict_token(mt, [p])[0] for p in prompts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing\n",
    "\n",
    "A natural next test is to try examining Casual Traces to see if it can distinguish knowledge retrieval from guessing behavior.\n",
    "\n",
    "The code below is taken from the causal tracing notebook from the ROME project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_with_patch(\n",
    "    model,            # The model\n",
    "    inp,              # A set of inputs\n",
    "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
    "    answers_t,        # Answer probabilities to collect\n",
    "    tokens_to_mix,    # Range of tokens to corrupt (begin, end)\n",
    "    noise=0.1,        # Level of noise to add\n",
    "    trace_layers=None # List of traced outputs to return\n",
    "):\n",
    "    prng = numpy.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    patch_spec = defaultdict(list)\n",
    "    for t, l in states_to_patch:\n",
    "        patch_spec[l].append(t)\n",
    "    embed_layername = layername(model, 0, 'embed')\n",
    "    \n",
    "    def untuple(x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer == embed_layername:\n",
    "            # If requested, we corrupt a range of token embeddings on batch items x[1:]\n",
    "            if tokens_to_mix is not None:\n",
    "                b, e = tokens_to_mix\n",
    "                x[1:, b:e] += noise * torch.from_numpy(\n",
    "                    prng.randn(x.shape[0] - 1, e - b, x.shape[2])\n",
    "                ).to(x.device)\n",
    "            return x\n",
    "        if layer not in patch_spec:\n",
    "            return x\n",
    "        # If this layer is in the patch_spec, restore the uncorrupted hidden state\n",
    "        # for selected tokens.\n",
    "        h = untuple(x)\n",
    "        for t in patch_spec[layer]:\n",
    "            h[1:, t] = h[0, t]\n",
    "        return x\n",
    "\n",
    "    # With the patching rules defined, run the patched model in inference.\n",
    "    additional_layers = [] if trace_layers is None else trace_layers\n",
    "    with torch.no_grad(), nethook.TraceDict(\n",
    "        model,\n",
    "        [embed_layername] +\n",
    "            list(patch_spec.keys()) + additional_layers,\n",
    "        edit_output=patch_rep\n",
    "    ) as td:\n",
    "        outputs_exp = model(**inp)\n",
    "\n",
    "    # We report softmax probabilities for the answers_t token predictions of interest.\n",
    "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
    "\n",
    "    # If tracing all layers, collect all activations together to return.\n",
    "    if trace_layers is not None:\n",
    "        all_traced = torch.stack(\n",
    "            [untuple(td[layer].output).detach().cpu() for layer in trace_layers], dim=2)\n",
    "        return probs, all_traced\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanning all locations\n",
    "\n",
    "A causal flow heatmap is created by repeating `trace_with_patch` at every individual hidden state, and measuring the impact of restoring state at each location.\n",
    "\n",
    "The `calculate_hidden_flow` function does this loop.  It handles both the case of restoring a single hidden state, and also restoring MLP or attention states.  Because MLP and attention make small residual contributions, to observe a causal effect in those cases, we need to restore several layers of contributions at once, which is done by `trace_important_window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hidden_flow(\n",
    "    mt, prompt, subject, samples=10, noise=0.1, window=10, kind=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs causal tracing over every token/layer combination in the network\n",
    "    and returns a dictionary numerically summarizing the results.\n",
    "    \"\"\"\n",
    "    inp = make_inputs(mt.tokenizer, [prompt] * (samples + 1))\n",
    "    with torch.no_grad():\n",
    "        answer_t, base_score = [d[0] for d in predict_from_input(mt.model, inp)]\n",
    "    [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "    e_range = find_token_range(mt.tokenizer, inp[\"input_ids\"][0], subject)\n",
    "    low_score = trace_with_patch(mt.model, inp, [], answer_t, e_range,\n",
    "            noise=noise).item()\n",
    "    if not kind:\n",
    "        differences = trace_important_states(\n",
    "            mt.model, mt.num_layers, inp, e_range, answer_t, noise=noise\n",
    "        )\n",
    "    else:\n",
    "        differences = trace_important_window(\n",
    "            mt.model,\n",
    "            mt.num_layers,\n",
    "            inp,\n",
    "            e_range,\n",
    "            answer_t,\n",
    "            noise=noise,\n",
    "            window=window,\n",
    "            kind=kind,\n",
    "        )\n",
    "    differences = differences.detach().cpu()\n",
    "    return dict(\n",
    "        scores=differences,\n",
    "        low_score=low_score,\n",
    "        high_score=base_score,\n",
    "        input_ids=inp[\"input_ids\"][0],\n",
    "        input_tokens=decode_tokens(mt.tokenizer, inp[\"input_ids\"][0]),\n",
    "        subject_range=e_range,\n",
    "        answer=answer,\n",
    "        window=window,\n",
    "        kind=kind or \"\",\n",
    "    )\n",
    "\n",
    "def trace_important_states(model, num_layers, inp, e_range, answer_t, noise=0.1):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    for tnum in range(ntoks):\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            r = trace_with_patch(\n",
    "                model,\n",
    "                inp,\n",
    "                [(tnum, layername(model, layer))],\n",
    "                answer_t,\n",
    "                tokens_to_mix=e_range,\n",
    "                noise=noise,\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)\n",
    "\n",
    "\n",
    "def trace_important_window(\n",
    "    model, num_layers, inp, e_range, answer_t, kind, window=10, noise=0.1\n",
    "):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    for tnum in range(ntoks):\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            layerlist = [\n",
    "                (tnum, layername(model, L, kind))\n",
    "                for L in range(\n",
    "                    max(0, layer - window // 2), min(num_layers, layer - (-window // 2))\n",
    "                )\n",
    "            ]\n",
    "            r = trace_with_patch(\n",
    "                model, inp, layerlist, answer_t, tokens_to_mix=e_range, noise=noise\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "The `plot_trace_heatmap` function draws the data on a heatmap.  That function is not shown here; it is in `experiments.causal_trace`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hidden_flow(\n",
    "    mt, prompt, subject=None, samples=10, noise=0.1, window=10, kind=None, modelname=None, savepdf=None\n",
    "):\n",
    "    if subject is None:\n",
    "        subject = guess_subject(prompt)\n",
    "    result = calculate_hidden_flow(\n",
    "        mt, prompt, subject, samples=samples, noise=noise, window=window, kind=kind\n",
    "    )\n",
    "    plot_trace_heatmap(result, savepdf, modelname=modelname)\n",
    "    \n",
    "def plot_all_flow(mt, prompt, subject=None, noise=0.1, modelname=None):\n",
    "    for kind in [None, \"mlp\", \"attn\"]:\n",
    "        plot_hidden_flow(mt, prompt, subject, modelname=modelname, noise=noise, kind=kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal traces of a few test cases\n",
    "\n",
    "Martha Stewart seems to reveal very clearly-defined knowledge retreival based on her name.\n",
    "\n",
    "On the other hand, Mark Allen does not at all.\n",
    "\n",
    "Beverly Cooper seems to be weakly predicted as an actress mainly based on her first name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_flow(mt, 'Martha Stewart, the well-known')\n",
    "plot_all_flow(mt, 'Beverly Cooper, the well-known')\n",
    "plot_all_flow(mt, 'Mark Allen, the well-known')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
