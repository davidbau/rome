{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidbau/rome/blob/gpt_stats/notebooks/statistics_of_GPT_states.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "cd /content && rm -rf /content/rome\n",
    "git clone https://github.com/davidbau/rome -b gpt_stats rome > install.log 2>&1\n",
    "pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "try:\n",
    "    import google.colab, torch, sys\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Change runtime type to include a GPU.\")\n",
    "    sys.path.append('/content/rome')\n",
    "    IS_COLAB = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of GPT states\n",
    "\n",
    "Here we load GPT-2-XL and examine the statistics of its internal states.\n",
    "\n",
    "We are particularly interested in: how close are the output values to Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build upon the ROME codebase.\n",
    "\n",
    "There is an `experiments.causal_trace` module that contains some useful utility functions for loading large transformer models in colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, re, json, math\n",
    "import torch, numpy\n",
    "from collections import defaultdict\n",
    "from experiments.causal_trace import ModelAndTokenizer, predict_token\n",
    "from baukit import show, nethook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load GPT-2-XL and a tokenizer, and show that it can complete a couple factual statements correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "mt = ModelAndTokenizer('gpt2-xl', low_cpu_mem_usage=IS_COLAB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some neutral text samples, we load some random texts from wikipedia.  Just a few (one batch for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from baukit import TokenizedDataset, length_collation, flatten_masked_batch\n",
    "\n",
    "def get_ds(ds_name='wikipedia', tokenizer=None, batch_tokens=None):\n",
    "    raw_ds = load_dataset(\n",
    "        ds_name,\n",
    "        dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name],\n",
    "    )\n",
    "    maxlen = mt.model.config.n_positions\n",
    "    if batch_tokens is not None and batch_tokens < maxlen:\n",
    "        maxlen = batch_tokens\n",
    "    return TokenizedDataset(raw_ds[\"train\"], tokenizer, maxlen=maxlen)\n",
    "ds = get_ds(tokenizer=mt.tokenizer, ds_name='wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.tokenizer.decode(ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    from baukit import show\n",
    "    item = ds[0]\n",
    "    cuda_batch = {k: v[None].cuda() for k, v in item.items()}\n",
    "    logits = mt.model(**cuda_batch)['logits']\n",
    "    probs = torch.nn.functional.softmax(logits, dim=2)\n",
    "    input_ids = item['input_ids']\n",
    "    preds = probs.argmax(dim=2)[0]\n",
    "    baseline_probs = probs[0, torch.arange(len(input_ids) - 1), input_ids[1:]]\n",
    "    show([[mt.tokenizer.decode(t), f'{p.item():.3g}', mt.tokenizer.decode(i), mt.tokenizer.decode(j)]\n",
    "      for t, i, p, j in zip(input_ids[:-1], input_ids[1:], baseline_probs, preds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set up a second copy in which the layer 17 MLP layer is randomized\n",
    "def make_perturbed_model():\n",
    "    mt2 = ModelAndTokenizer('gpt2-xl', low_cpu_mem_usage=IS_COLAB)\n",
    "    for layernum in range(10, 11):\n",
    "        R = torch.randn(6400, 1600)\n",
    "        RU, RS, RV = R.svd()\n",
    "        orig_W = mt2.model.transformer.h[layernum].mlp.c_proj.weight.detach().cpu()\n",
    "        U, S, V = orig_W.svd()\n",
    "        randomized_W = U @ torch.diag(S) @ RV.t()\n",
    "        mt2.model.transformer.h[layernum].mlp.c_proj.weight[...] = randomized_W\n",
    "    return mt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt2 = make_perturbed_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise_const = torch.from_numpy(numpy.random.RandomState(1).randn(1024 * 1024)).float().cuda()\n",
    "PLAIN = show.style()\n",
    "RED = show.style(background='red')\n",
    "BLUE = show.style(background='skyblue')\n",
    "HEADING = show.style(fontWeight='bold', borderBottom='1px solid black')\n",
    "\n",
    "def filter_and_print(mt, input_ids, baseline_probs, preds, n_baseline_probs, n_preds, threshold=5.0, docid=None):\n",
    "    gt = input_ids[1:].cuda()\n",
    "    new_mistakes = (gt == preds[:-1]) & (gt != n_preds[:-1]) & (gt != 198) & (\n",
    "        n_baseline_probs * threshold < baseline_probs)\n",
    "    if not new_mistakes.sum():\n",
    "        return\n",
    "    if docid is not None:\n",
    "        print(f'docid {docid}')\n",
    "        \n",
    "    show(\n",
    "        [HEADING, ['tok', 'gtprob', 'corrupted', 'gt', 'gti', 'pred', 'corrupted']] +\n",
    "         [[show.Tag('p'), mt.tokenizer.decode(t),\n",
    "           f'{p.item():.3g}',\n",
    "           (RED if np.item() * threshold < p.item() else PLAIN), f'{np.item():.3g}',\n",
    "           (RED if new_mistakes[ind] else PLAIN), mt.tokenizer.decode(i),\n",
    "           i.item(),\n",
    "           mt.tokenizer.decode(j),\n",
    "           mt.tokenizer.decode(nj)]\n",
    "      for ind, (t, i, p, j, np, nj) in enumerate(\n",
    "          zip(input_ids[:-1],\n",
    "              input_ids[1:],\n",
    "              baseline_probs,\n",
    "              preds,\n",
    "              n_baseline_probs,\n",
    "              n_preds))\n",
    "          if new_mistakes[ind]\n",
    "         ]\n",
    "    )\n",
    "    \n",
    "    buf = ['<pre>']\n",
    "    for ind, t in enumerate(input_ids):\n",
    "        if ind > 0 and new_mistakes[ind - 1]:\n",
    "            buf.append('<b style=\"color:red\">')\n",
    "            buf.append(mt.tokenizer.decode(t))\n",
    "            buf.append('</b>')\n",
    "        else:\n",
    "            buf.append(mt.tokenizer.decode(t))\n",
    "    buf.append('</pre>')\n",
    "    show(show.raw_html(''.join(buf)))\n",
    "\n",
    "\n",
    "def run_perturbed(datum, threshold=5.0, docid=None):\n",
    "    maxlen = mt.model.config.n_positions\n",
    "    input_ids = datum['input_ids'][:maxlen]\n",
    "    cuda_batch = {k: v[None][:maxlen].cuda() for k, v in datum.items()}\n",
    "    logits = mt.model(**cuda_batch)['logits']\n",
    "    probs = torch.nn.functional.softmax(logits, dim=2)\n",
    "    preds = probs.argmax(dim=2)[0]\n",
    "    baseline_probs = probs[0, torch.arange(len(input_ids) - 1), input_ids[1:]]\n",
    "    n_logits = mt2.model(**cuda_batch)['logits']\n",
    "    n_probs = torch.nn.functional.softmax(n_logits, dim=2)\n",
    "    n_preds = n_probs.argmax(dim=2)[0]\n",
    "    n_baseline_probs = n_probs[0, torch.arange(len(input_ids) - 1), input_ids[1:]]\n",
    "    filter_and_print(mt, input_ids, baseline_probs, preds, n_baseline_probs, n_preds, threshold=threshold, docid=docid)\n",
    "    \n",
    "def old_filter_and_print(mt, input_ids, baseline_probs, preds, n_baseline_probs, n_preds, threshold=5.0, docid=None):\n",
    "    gt = input_ids[1:].cuda()\n",
    "    new_mistakes = (gt == preds[:-1]) & (gt != n_preds[:-1])\n",
    "    #if new_mistakes.sum() and (baseline_probs / n_baseline_probs).max() > threshold:\n",
    "    if new_mistakes.sum() and (baseline_probs[new_mistakes] / n_baseline_probs[new_mistakes]).max() > threshold:\n",
    "        last_index = ((baseline_probs / n_baseline_probs) > threshold).nonzero()[-1, 0] + 5\n",
    "        show.bare(show.style(display='flex'), [\n",
    "            [HEADING, ['tok', 'gtprob', 'corrupted', 'gt', 'pred', 'corrupted']] +\n",
    "             [[show.Tag('p'), mt.tokenizer.decode(t),\n",
    "               f'{p.item():.3g}',\n",
    "               (RED if np.item() * threshold < p.item() else PLAIN), f'{np.item():.3g}',\n",
    "               (RED if new_mistakes[ind] else PLAIN), mt.tokenizer.decode(i),\n",
    "               mt.tokenizer.decode(j),\n",
    "               mt.tokenizer.decode(nj)]\n",
    "          for ind, (t, i, p, j, np, nj) in enumerate(\n",
    "              zip(input_ids[:-1],\n",
    "                  input_ids[1:last_index],\n",
    "                  baseline_probs,\n",
    "                  preds,\n",
    "                  n_baseline_probs,\n",
    "                  n_preds))]\n",
    "        ])\n",
    "#run_perturbed(ds[7], threshold=2.0, docid=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for doc in range(0, 10000):\n",
    "    run_perturbed(ds[doc], threshold=5.0, docid=doc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
