{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidbau/rome/blob/gpt_stats/notebooks/statistics_of_GPT_states.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "cd /content && rm -rf /content/rome\n",
    "git clone https://github.com/davidbau/rome -b gpt_stats rome > install.log 2>&1\n",
    "pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "try:\n",
    "    import google.colab, torch, sys\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Change runtime type to include a GPU.\")\n",
    "    sys.path.append('/content/rome')\n",
    "    IS_COLAB = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of GPT states\n",
    "\n",
    "Here we load GPT-2-XL and examine the statistics of its internal states.\n",
    "\n",
    "We are particularly interested in: how close are the output values to Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build upon the ROME codebase.\n",
    "\n",
    "There is an `experiments.causal_trace` module that contains some useful utility functions for loading large transformer models in colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, re, json, math\n",
    "import torch, numpy\n",
    "from collections import defaultdict\n",
    "from experiments.causal_trace import ModelAndTokenizer, predict_token\n",
    "from baukit import show, nethook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load GPT-2-XL and a tokenizer, and show that it can complete a couple factual statements correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "mt = ModelAndTokenizer('gpt2-xl', low_cpu_mem_usage=IS_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some neutral text samples, we load some random texts from wikipedia.  Just a few (one batch for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from baukit import TokenizedDataset, length_collation, flatten_masked_batch\n",
    "\n",
    "def get_ds(ds_name='wikipedia', tokenizer=None, batch_tokens=None):\n",
    "    raw_ds = load_dataset(\n",
    "        ds_name,\n",
    "        dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name],\n",
    "    )\n",
    "    maxlen = mt.model.config.n_positions\n",
    "    if batch_tokens is not None and batch_tokens < maxlen:\n",
    "        maxlen = batch_tokens\n",
    "    return TokenizedDataset(raw_ds[\"train\"], tokenizer, maxlen=maxlen)\n",
    "ds = get_ds(tokenizer=mt.tokenizer, ds_name='wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.tokenizer.decode(ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    from baukit import show\n",
    "    item = ds[0]\n",
    "    cuda_batch = {k: v[None].cuda() for k, v in item.items()}\n",
    "    logits = mt.model(**cuda_batch)['logits']\n",
    "    probs = torch.nn.functional.softmax(logits, dim=2)\n",
    "    input_ids = item['input_ids']\n",
    "    preds = probs.argmax(dim=2)[0]\n",
    "    baseline_probs = probs[0, torch.arange(len(input_ids) - 1), input_ids[1:]]\n",
    "    show([[mt.tokenizer.decode(t), f'{p.item():.3g}', mt.tokenizer.decode(i), mt.tokenizer.decode(j)]\n",
    "      for t, i, p, j in zip(input_ids[:-1], input_ids[1:], baseline_probs, preds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0, :10].sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise_const = torch.from_numpy(numpy.random.RandomState(1).randn(1024 * 1024)).float().cuda()\n",
    "PLAIN = show.style()\n",
    "RED = show.style(background='red')\n",
    "BLUE = show.style(background='skyblue')\n",
    "HEADING = show.style(fontWeight='bold', borderBottom='1px solid black')\n",
    "\n",
    "def run_with_noise(datum, index, amplitude=0.1):\n",
    "    maxlen = mt.model.config.n_positions\n",
    "    input_ids = datum['input_ids'][:maxlen]\n",
    "    cuda_batch = {k: v[None][:maxlen].cuda() for k, v in datum.items()}\n",
    "    logits = mt.model(**cuda_batch)['logits']\n",
    "    probs = torch.nn.functional.softmax(logits, dim=2)\n",
    "    preds = probs.argmax(dim=2)[0]\n",
    "    baseline_probs = probs[0, torch.arange(len(input_ids) - 1), input_ids[1:]]\n",
    "    def add_noise(x):\n",
    "        numel = x[:, index].numel()\n",
    "        shape = x[:, index].shape\n",
    "        x[:,index] += amplitude * noise_const[:numel].view(shape)\n",
    "    with nethook.Trace(mt.model, 'transformer.wte', edit_output=add_noise):\n",
    "        n_logits = mt.model(**cuda_batch)['logits']\n",
    "    n_probs = torch.nn.functional.softmax(n_logits, dim=2)\n",
    "    n_preds = probs.argmax(dim=2)[0]\n",
    "    n_baseline_probs = n_probs[0, torch.arange(len(input_ids) - 1), input_ids[1:]]\n",
    "    if (baseline_probs / n_baseline_probs)[index + 5:].max() > 4.0:\n",
    "        last_index = ((baseline_probs / n_baseline_probs) > 4.0).nonzero()[-1, 0] + 5\n",
    "        show.bare(show.style(display='flex'), [[HEADING, ['tok', 'gtprob', 'corrupted', 'gt', 'pred', 'corrupted']] +\n",
    "             [[show.Tag('p'), (BLUE if ind == index else PLAIN), mt.tokenizer.decode(t),\n",
    "               f'{p.item():.3g}',\n",
    "               (RED if np.item() * 4 < p.item() else PLAIN), f'{np.item():.3g}',\n",
    "               (RED if np.item() * 4 < p.item() else PLAIN), mt.tokenizer.decode(i),\n",
    "               mt.tokenizer.decode(j),\n",
    "               mt.tokenizer.decode(nj)]\n",
    "          for ind, (t, i, p, j, np, nj) in enumerate(zip(input_ids[:-1], input_ids[1:last_index], baseline_probs, preds, n_baseline_probs, n_preds))]])\n",
    "run_with_noise(ds[7], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for doc in range(10):\n",
    "    for tok in range(10):\n",
    "        run_with_noise(ds[doc], tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show.bare(1,[2,3],3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
